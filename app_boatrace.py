import streamlit as st
import pandas as pd
import numpy as np
import lightgbm as lgb
import requests
from bs4 import BeautifulSoup
import datetime
import os
import re
import time

# --- Configuration ---
MODEL_PATH = 'lgb_ranker.txt'
DATA_DIR = 'app_data'

# Default Headers for Scraping
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# --- 1. Scraper Class (Synchronous Adaptation of data_fetcher.py) ---
class BoatRaceScraper:
    @staticmethod
    def get_soup(url):
        try:
            resp = requests.get(url, headers=HEADERS, timeout=10)
            resp.raise_for_status()
            resp.encoding = resp.apparent_encoding
            return BeautifulSoup(resp.text, 'html.parser')
        except Exception as e:
            st.error(f"Error fetching URL {url}: {e}")
            return None

    @staticmethod
    def parse_float(text):
        try:
            return float(re.search(r'([\d\.]+)', text).group(1))
        except:
            return 0.0

    @staticmethod
    def get_race_data(date_str, venue_code, race_no):
        jcd = f"{int(venue_code):02d}"
        
        # URLs
        url_before = f"https://www.boatrace.jp/owpc/pc/race/beforeinfo?rno={race_no}&jcd={jcd}&hd={date_str}"
        url_list = f"https://www.boatrace.jp/owpc/pc/race/racelist?rno={race_no}&jcd={jcd}&hd={date_str}"
        
        soup_before = BoatRaceScraper.get_soup(url_before)
        soup_list = BoatRaceScraper.get_soup(url_list)
        
        if not soup_before or not soup_list:
            return None
            
        # --- Parse Before Info (Weather & ST) ---
        weather_info = {
            'wind_direction': 0, 'wind_speed': 0.0, 'wave_height': 0.0, 
            'temperature': 0.0, 'water_temp': 0.0
        }
        
        try:
            w_el = soup_before.select_one("div.weather1_body")
            if w_el:
                # Wind Dir
                wd_p = w_el.select_one(".is-windDirection p")
                if wd_p:
                    cls = wd_p.get('class', [])
                    dir_cls = next((c for c in cls if c.startswith('is-wind') and c != 'is-windDirection'), None)
                    if dir_cls:
                        weather_info['wind_direction'] = int(re.sub(r'\D', '', dir_cls))
                
                # Wind Speed
                ws_span = w_el.select_one(".is-wind span.weather1_bodyUnitLabelData")
                if ws_span: weather_info['wind_speed'] = BoatRaceScraper.parse_float(ws_span.text)
                
                # Wave
                wh_span = w_el.select_one(".is-wave span.weather1_bodyUnitLabelData")
                if wh_span: weather_info['wave_height'] = BoatRaceScraper.parse_float(wh_span.text)
                
        except Exception as e:
            st.warning(f"Weather parse error: {e}")

        # Parse ST & Exhibition
        boat_before_data = {}
        try:
            tbody_st = soup_before.select("table.is-w238 tbody tr") # ST Table
            for row in tbody_st:
                bn_span = row.select_one("span.table1_boatImage1Number")
                if bn_span:
                    bn = int(bn_span.text.strip())
                    st_span = row.select_one("span.table1_boatImage1Time")
                    st_val = 0.0
                    if st_span:
                        txt = st_span.text.strip().replace('F', '-0.') # F.01 -> -0.01
                        # Simple F handling: just negative
                        if 'F' in st_span.text:
                             st_val = -0.05 # Penalty assumption
                        elif 'L' in st_span.text:
                             st_val = 1.0 # Late
                        else:
                             st_val = float(txt)
                    boat_before_data[bn] = {'st': st_val}
            
            # Exhibition Time (Main Table)
            tbody_ex = soup_before.select("table.is-w748 tbody")
            for i, tb in enumerate(tbody_ex):
                bn = i + 1
                # Exhibition time is usually 5th col
                tds = tb.select("td")
                if len(tds) >= 5:
                    ex_time = BoatRaceScraper.parse_float(tds[4].text)
                    if bn in boat_before_data:
                        boat_before_data[bn]['ex_time'] = ex_time
                    else:
                        boat_before_data[bn] = {'st': 0.20, 'ex_time': ex_time}
                        
        except Exception as e:
            st.warning(f"Before Info parse error: {e}")
            # Fallback
            for i in range(1, 7):
                if i not in boat_before_data: boat_before_data[i] = {'st': 0.20, 'ex_time': 6.8}

        # --- Parse Race List (Racer Info) ---
        rows = []
        try:
            tbodies = soup_list.select("tbody.is-fs12")
            for i, tb in enumerate(tbodies):
                bn = i + 1
                if bn > 6: break
                
                # Racer ID
                racer_id = 9999
                try:
                    # 3rd td usually has racer info
                    td_racer = tb.select("td")[2] 
                    div_top = td_racer.select_one("div")
                    if div_top:
                        txt = div_top.get_text()
                        match = re.search(r'(\d{4})', txt)
                        if match: racer_id = int(match.group(1))
                except: pass
                
                # Motor Rate
                motor_rate = 30.0
                try:
                    td_motor = tb.select("td")[6]
                    txt = td_motor.get_text(separator='|')
                    parts = txt.split('|')
                    if len(parts) >= 2:
                        motor_rate = BoatRaceScraper.parse_float(parts[1])
                except: pass
                
                # Boat Rate
                boat_rate = 30.0
                try:
                    td_boat = tb.select("td")[7]
                    txt = td_boat.get_text(separator='|')
                    parts = txt.split('|')
                    if len(parts) >= 2:
                        boat_rate = BoatRaceScraper.parse_float(parts[1])
                except: pass
                
                row = {
                    'race_id': f"{date_str}_{venue_code}_{race_no}",
                    'boat_number': bn,
                    'racer_id': racer_id,
                    'motor_rate': motor_rate,
                    'boat_rate': boat_rate,
                    'exhibition_time': boat_before_data.get(bn, {}).get('ex_time', 6.8),
                    'exhibition_start_timing': boat_before_data.get(bn, {}).get('st', 0.20),
                    'pred_course': bn, # Default to frame
                    # Weather
                    'wind_direction': weather_info['wind_direction'],
                    'wind_speed': weather_info['wind_speed'],
                    'wave_height': weather_info['wave_height'],
                    # Dummy for now (Needs robust parsing or API)
                    'prior_results': "3.5", 
                    'nige_count': 0, 'makuri_count': 0 # Will be filled by static data if available
                }
                rows.append(row)
        except Exception as e:
            st.error(f"Race List parse error: {e}")
            return None
            
        return pd.DataFrame(rows)

# --- 2. Feature Engineer ---
class FeatureEngineer:
    @staticmethod
    def process(df, venue_name):
        # Load Static Data
        try:
            r_course = pd.read_csv(os.path.join(DATA_DIR, 'static_racer_course.csv'))
            r_venue = pd.read_csv(os.path.join(DATA_DIR, 'static_racer_venue.csv'))
            v_course = pd.read_csv(os.path.join(DATA_DIR, 'static_venue_course.csv'))
            r_params = pd.read_csv(os.path.join(DATA_DIR, 'static_racer_params.csv'))
            
            # Type Casting
            df['racer_id'] = df['racer_id'].astype(int)
            df['pred_course'] = df['pred_course'].astype(int)
            
            # Merges
            df = df.merge(r_course, left_on=['racer_id', 'pred_course'], right_on=['RacerID', 'Course'], how='left')
            df = df.merge(r_venue, left_on=['racer_id'], right_on=['RacerID'], how='left') 
            # Note: r_venue usually has Venue col, but we just average if static data is simplified. 
            # Or if static_racer_venue has 'Venue' col, we filter?
            # For this demo app compliance, we assume static_racer_venue is PRE-FILTERED or pivot?
            # Actually, static_racer_venue.csv exported by user likely contains ALL. 
            # Let's check columns if we can. Assuming simple merge for now.
            
            df = df.merge(v_course, left_on=['pred_course'], right_on=['course_number'], how='left')
            # Again, need to filter v_course by venue_name?
            # The static_venue_course.csv likely has venue_name.
            
            df = df.merge(r_params, on='racer_id', how='left')
            
            df = df.fillna(0)
            
        except Exception as e:
            st.warning(f"Static data merge error: {e}. Using raw features only.")
        
        # --- Wind Vector ---
        direction_map = {
            1: 0, 2: 45, 3: 45, 4: 90, 5: 90, 6: 135,
            7: 135, 8: 180, 9: 180, 10: 225, 11: 225, 12: 270,
            13: 270, 14: 315, 15: 315, 16: 0
        }
        # Note: Scraper returns 1-16. 
        # 1=North(0), 5=East(90), 9=South(180), 13=West(270)
        
        venue_tailwind_from = {
             'Ê°êÁîü': 135, 'Êà∏Áî∞': 90, 'Ê±üÊà∏Â∑ù': 180, 'Âπ≥ÂíåÂ≥∂': 180, 'Â§öÊë©Â∑ù': 270,
             'ÊµúÂêçÊπñ': 180, 'Ëí≤ÈÉ°': 270, 'Â∏∏Êªë': 270, 'Ê¥•': 135, '‰∏âÂõΩ': 180,
             '„Å≥„Çè„Åì': 225, '‰Ωè‰πãÊ±ü': 270, 'Â∞ºÂ¥é': 90, 'È≥¥ÈñÄ': 135, '‰∏∏‰∫Ä': 180,
             'ÂÖêÂ≥∂': 225, 'ÂÆÆÂ≥∂': 270, 'Âæ≥Â±±': 135, '‰∏ãÈñ¢': 270, 'Ëã•Êùæ': 270,
             'Ëä¶Â±ã': 135, 'Á¶èÂ≤°': 0, 'ÂîêÊ¥•': 135, 'Â§ßÊùë': 315
        }
        
        df['wind_angle_deg'] = df['wind_direction'].map(direction_map).fillna(0)
        tailwind_deg = venue_tailwind_from.get(venue_name, 0)
        
        angle_diff_rad = np.radians(df['wind_angle_deg'] - tailwind_deg)
        df['wind_vector_long'] = df['wind_speed'] * np.cos(angle_diff_rad)
        df['wind_vector_lat'] = df['wind_speed'] * np.sin(angle_diff_rad)
        
        # --- Relative Stats ---
        # Sort by Boat Number
        df = df.sort_values('boat_number')
        
        # Inner/Outer ST
        df['inner_st'] = df['exhibition_start_timing'].shift(1).fillna(0)
        df['outer_st'] = df['exhibition_start_timing'].shift(-1).fillna(0)
        df['slit_formation'] = df['exhibition_start_timing'] - ((df['inner_st']+df['outer_st'])/2)
        
        # Tenji Z-score
        mean_t = df['exhibition_time'].mean()
        std_t = df['exhibition_time'].std()
        if std_t == 0: std_t = 1.0
        df['tenji_z_score'] = (mean_t - df['exhibition_time']) / std_t
        
        # Mocking missing columns expected by model if they don't exist
        expected_cols = [
            'series_avg_rank', 'makuri_rate', 'nige_rate', 'inner_st_gap', 
            'anti_nige_potential', 'wall_strength', 'follow_potential'
        ]
        for c in expected_cols:
            if c not in df.columns: df[c] = 0.0
            
        return df

# --- 3. Main App ---
st.set_page_config(page_title="BoatRace AI Predictor", layout="wide")

st.title("üö§ BoatRace AI Strategy: 'Structure & Value'")
st.markdown("Returns-Focused AI Prediction System")

# Sidebar
st.sidebar.header("Race Selection")
target_date = st.sidebar.date_input("Date", datetime.date.today())
venue_map = {
    1: 'Ê°êÁîü', 2: 'Êà∏Áî∞', 3: 'Ê±üÊà∏Â∑ù', 4: 'Âπ≥ÂíåÂ≥∂', 5: 'Â§öÊë©Â∑ù',
    6: 'ÊµúÂêçÊπñ', 7: 'Ëí≤ÈÉ°', 8: 'Â∏∏Êªë', 9: 'Ê¥•', 10: '‰∏âÂõΩ',
    11: '„Å≥„Çè„Åì', 12: '‰Ωè‰πãÊ±ü', 13: 'Â∞ºÂ¥é', 14: 'È≥¥ÈñÄ', 15: '‰∏∏‰∫Ä',
    16: 'ÂÖêÂ≥∂', 17: 'ÂÆÆÂ≥∂', 18: 'Âæ≥Â±±', 19: '‰∏ãÈñ¢', 20: 'Ëã•Êùæ',
    21: 'Ëä¶Â±ã', 22: 'Á¶èÂ≤°', 23: 'ÂîêÊ¥•', 24: 'Â§ßÊùë'
}
venue_code = st.sidebar.selectbox("Venue", list(venue_map.keys()), format_func=lambda x: f"{x:02d}: {venue_map[x]}")
venue_name = venue_map[venue_code]
race_no = st.sidebar.selectbox("Race No", range(1, 13))

if st.button("Analyze Race", type="primary"):
    date_str = target_date.strftime('%Y%m%d')
    st.info(f"Fetching Data: {venue_name} {race_no}R ({date_str})")
    
    # 1. Scrape
    with st.spinner("Scraping official website..."):
        df_race = BoatRaceScraper.get_race_data(date_str, venue_code, race_no)
    
    if df_race is not None:
        # Show Live Data
        st.subheader("Live Race Data")
        cols = ['boat_number', 'racer_id', 'motor_rate', 'boat_rate', 'exhibition_time', 'exhibition_start_timing', 'wind_speed', 'wave_height']
        st.dataframe(df_race[cols])
        
        # 2. Feature Engineering
        with st.spinner("Processing AI Features..."):
            df_features = FeatureEngineer.process(df_race, venue_name)
            
        # 3. Prediction
        with st.spinner("Running LightGBM Inference..."):
            try:
                model = lgb.Booster(model_file=MODEL_PATH)
                preds = model.predict(df_features.drop(columns=['race_id'], errors='ignore'))
                df_features['score'] = preds
            except Exception as e:
                st.error(f"Prediction Error: {e}")
                # Fallback score
                df_features['score'] = df_features['motor_rate'] 
                
        # 4. Display Results
        st.divider()
        st.subheader("ü§ñ AI Prediction Ranking")
        
        rank_df = df_features[['boat_number', 'score']].sort_values('score', ascending=False)
        rank_df['rank'] = range(1, 7)
        st.dataframe(rank_df.set_index('rank'))
        
        # 5. Betting Strategy (Top 5 Pattern)
        st.subheader("üéØ Recommended Strategy (Top 5 Pattern)")
        
        boats = rank_df['boat_number'].tolist()
        # Simple Permutations of Top Boats? 
        # Actually Pattern A (Top 5 Combinations) requires Combination Scoring.
        # Approx: Just showing Top 3 boats Box?
        # Or showing Top 5 combinations derived from Score?
        # Score_Combo = Score(1) * Score(2) * Score(3)
        import itertools
        combos = list(itertools.permutations(boats, 3)) # 120 combos sorted by boat priority?
        # No, we need to sort combos by Sum/Product of scores.
        
        combo_scores = []
        boat_score_map = dict(zip(rank_df['boat_number'], rank_df['score']))
        
        for c in combos:
            s = boat_score_map[c[0]] * boat_score_map[c[1]] * boat_score_map[c[2]]
            combo_scores.append({'combo': f"{c[0]}-{c[1]}-{c[2]}", 'score': s})
            
        combo_df = pd.DataFrame(combo_scores).sort_values('score', ascending=False).head(5)
        
        st.success("‚úÖ BUY THESE 5 POINTS (Flat Bet)")
        for idx, row in combo_df.iterrows():
            st.metric(label=f"Rank {idx+1}", value=row['combo'])
            
    else:
        st.error("Failed to load race data. Race might be invalid or cancelled.")
